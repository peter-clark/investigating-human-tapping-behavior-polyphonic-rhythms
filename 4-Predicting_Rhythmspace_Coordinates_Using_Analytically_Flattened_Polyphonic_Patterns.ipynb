{"cells":[{"cell_type":"markdown","metadata":{"id":"D4MwW62Zc64O"},"source":["Pattern -> Descriptors -> Embeddings -> Coordinates (by patt index)\n","\n","  -->  MIDI Drum Assignment -> Notes In Channels -> Flattening Type Results (by patt index) ==> for all flattening types.\n","  \n","---\n","Build and test NN for each (flat, coord) combo & report MAE results."]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1708014957745,"user":{"displayName":"Peter Clark","userId":"17011029620099952771"},"user_tz":540},"id":"I11H7iCGf1c2"},"outputs":[],"source":["## Imports ##\n","import os\n","import numpy as np\n","import sys\n","import random\n","import importlib\n","\n","## TODO: double check these need to be imported in this file\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.utils.data as tdata\n","import torch.backends.mps\n","from sklearn.model_selection import cross_val_score\n","from sklearn.model_selection import KFold\n","\n","import matplotlib.pyplot as plt\n","from sklearn.model_selection import train_test_split"]},{"cell_type":"code","execution_count":28,"metadata":{"id":"6EIe1ZYIiyJw"},"outputs":[],"source":["## File Locations (GitHub repo dir) ##\n","dir=os.getcwd()\n","os.listdir(dir)\n","\n","import descriptors as desc\n","import functions as func\n","import flatten as flat\n","import neuralnetwork"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8706,"status":"ok","timestamp":1708015502925,"user":{"displayName":"Peter Clark","userId":"17011029620099952771"},"user_tz":540},"id":"kPQtxQZli3VA","outputId":"bb2b3036-d6eb-4b5f-a3c5-a7d43cfebad6"},"outputs":[{"data":{"text/plain":["\"\\nMounting my drive doesn't work by code, but does by side panel.\\nWe will need to figure this\\n\\ndir = os.chdir('/content/drive/MyDrive/Tapping_Force_Experiment_Data')\\nos.listdir(dir)\\nimport descriptors as desc\\nimport functions as func\\n\""]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["## File Locations (Google Drive) ##\n","#drive.mount('/content/drive/MyDrive/Tapping_Force_Experiment_Data')\n","\n","'''\n","Mounting my drive doesn't work by code, but does by side panel.\n","We will need to figure this\n","\n","dir = os.chdir('/content/drive/MyDrive/Tapping_Force_Experiment_Data')\n","os.listdir(dir)\n","import descriptors as desc\n","import functions as func\n","'''"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"zHOWGHjBf5CM"},"outputs":[],"source":["## Patterns ##\n","all_pattlists, all_names = func.rootfolder2pattlists(\"midi/\",\"allinstruments\")\n"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[],"source":["## Descriptors & Embedding Coordinates ~23s ##\n","d = desc.lopl2descriptors(all_pattlists)\n","\n","mds_pos = func.d_mtx_2_mds(d)"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"c-iDSXBJgM-z"},"outputs":[{"name":"stdout","output_type":"stream","text":["[[1], [], [2], [], [1], [], [2], [], [1], [], [2], [], [1], [], [2], []]\n"]}],"source":["## Convert Patterns (Full MIDI --> (8?) 3-Channel) ##\n","patterns = [] # 3 channel patterns\n","importlib.reload(flat)\n","\n","for patt in all_pattlists:\n","    p = flat.get_LMH(patt)\n","    patterns.append(p)\n","print(patterns[1])"]},{"cell_type":"markdown","metadata":{},"source":["### Analytical Flattening Types:\n","\n","Types: | Combo: | FuncRef: | Index: |\n","|---|:---:|---|---:|\n","**Onset Density** | (only) | [0,0,0] | 0 |\n","&emsp; &emsp; +| Metrical Strength  | [0,1,0] | 1 |\n","&emsp; &emsp; +| Syncopation  | [0,0,1] | 2 |\n","&emsp; &emsp; +| Metrical Strength <br>& Syncopation  | [0,1,1] | 3 |\n","--- | --- | --- | ---\n","**Frequency Weighted <br>Onset Density** | (only) | [1,0,0] | 4 |\n","&emsp; &emsp; +| Metrical Strength  | [1,1,0] | 5 |\n","&emsp; &emsp; +| Syncopation  | [1,0,1] | 6 |\n","&emsp; &emsp; +| Metrical Strength <br>& Syncopation  | [1,1,1] | 7 |\n","--- | --- | --- | ---\n","**Pattern Relative <br>Onset Density** | (only) | [2,0,0] | 8 |\n","&emsp; &emsp; +| Metrical Strength  | [2,1,0] | 9 |\n","&emsp; &emsp; +| Syncopation  | [2,0,1] | 10 |\n","&emsp; &emsp; +| Metrical Strength <br>& Syncopation  | [2,1,1] | 11 |\n","--- | --- | --- | ---\n","**Note Presence** | (only/mono) | [3,0,0] | 12 |\n","&emsp; &emsp; +| Metrical Strength  | [3,1,0] | 13 |\n","&emsp; &emsp; +| Syncopation  | [3,0,1] | 14 |\n","&emsp; &emsp; +| Metrical Strength <br>& Syncopation  | [3,1,1] | 15 |\n","&emsp; &emsp; +| Polyphonic Syncopation  | [3,0,2] | 16 |\n","\n","\n"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["# density types\n","onset_density=0\n","freq_weighted_onset_density=1\n","relative_onset_density=2\n","note_presence=3\n","# meter\n","no_meter=0\n","meter=1\n","# sync\n","no_sync=0\n","sync=1\n","polysync=2"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"L4in3229guN8"},"outputs":[],"source":["importlib.reload(flat)\n","## Convert 3-channel to all 17 flat types. ##\n","\n","n_patterns = len(patterns)\n","types = 17\n","flat_names = ['OnsetDen', 'OnsetDen_Meter', 'OnsetDen_Sync','OnsetDen_Sync_Meter',\n","              'FreqWeightOnsetDen', 'FreqWeightOnsetDen_Meter', 'FreqWeightOnsetDen_Sync','FreqWeightOnsetDen_Sync_Meter',\n","              'RelOnsetDen', 'RelOnsetDen_Meter', 'RelOnsetDen_Sync','RelOnsetDen_Sync_Meter',\n","              'Mono', 'Mono_Meter', 'Mono_Sync','Mono_Sync_Meter', 'Mono_PolySync']\n","len_patt = 16\n","\n","# test\n","# patterns=patterns[13:20]\n","\n","flatterns = np.array([[[0.0 for z in range(len_patt)] for y in range(types)] for x in range(n_patterns)], dtype=float) \n","for p in range(len(patterns)):\n","    pattern = patterns[p]\n","# Onset Density\n","    flatterns[p][0] = flat.flat(pattern, onset_density, no_meter, no_sync)\n","    flatterns[p][1] = flat.flat(pattern, onset_density, meter, no_sync)\n","    flatterns[p][2] = flat.flat(pattern, onset_density, no_meter, sync)\n","    flatterns[p][3] = flat.flat(pattern, onset_density, meter, sync)\n","# Frequency Weighted Onset Density\n","    flatterns[p][4] = flat.flat(pattern, freq_weighted_onset_density, no_meter, no_sync)\n","    flatterns[p][5] = flat.flat(pattern, freq_weighted_onset_density, meter, no_sync)\n","    flatterns[p][6] = flat.flat(pattern, freq_weighted_onset_density, no_meter, sync)\n","    flatterns[p][7] = flat.flat(pattern, freq_weighted_onset_density, meter, sync)\n","# Pattern Relatiive Onset Density\n","    flatterns[p][8] = flat.flat(pattern, relative_onset_density, no_meter, no_sync)\n","    flatterns[p][9] = flat.flat(pattern, relative_onset_density, meter, no_sync)\n","    flatterns[p][10] = flat.flat(pattern, relative_onset_density, no_meter, sync)\n","    flatterns[p][11] = flat.flat(pattern, relative_onset_density, meter, sync)\n","# Note Presence\n","    flatterns[p][12] = flat.flat(pattern, note_presence, no_meter, no_sync)\n","    flatterns[p][13] = flat.flat(pattern, note_presence, meter, no_sync)\n","    flatterns[p][14] = flat.flat(pattern, note_presence, no_meter, sync)\n","    flatterns[p][15] = flat.flat(pattern, note_presence, meter, sync)\n","    flatterns[p][16] = flat.flat(pattern, note_presence, no_meter, polysync)\n","    if p %100==0:\n","        print(f\"[x] - {p}/{len(patterns)}\")\n","print(f\"[x] - {p+1}/{len(patterns)}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HWG-MlfXhHSO"},"outputs":[],"source":["''' CHANGE THIS TO DESIRED FLATTENING TYPE'''\n","flat_type = 17\n","\n","\"\"\" print(flatterns.shape)\n","flattern = flatterns[:,flat_type,:]\n","print(flat_names[flat_type])\n","print(np.round(flattern[13:19],4)) \"\"\"\n","\n","for i in range(flat_type):\n","    print(flatterns.shape)\n","    flattern = flatterns[:,i,:]\n","    print(flat_names[i])\n","    print(flattern[824]) # example"]},{"cell_type":"code","execution_count":102,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Using cpu device\n"]}],"source":["## Load? ##\n","_load = False\n","model_dir=''\n","importlib.reload(neuralnetwork)\n","\n","## Define model structure ##\n","model_layers=4\n","len_pattern=16 # input layer\n","firstlayer=32\n","secondlayer=32\n","thirdlayer=16\n","fourthlayer=16\n","coords=2 # output layer\n","\n","## Build / Load model ##\n","if _load:\n","    model=neuralnetwork.load_model(model_dir)\n","else:\n","    model = neuralnetwork.build_model(len_pattern=16,firstlayer=32,secondlayer=32,thirdlayer=16,fourthlayer=16,coords=2)\n","\n","## Select model hyperparameters ##\n","learning_rate = 0.002\n","batch_size = 32\n","criterion = nn.MSELoss()\n","optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n","epochs = 300\n","\n","train_loss = 0.0\n","test_loss = 0.0"]},{"cell_type":"markdown","metadata":{},"source":["### Analytical Flattening Types:\n","\n","Types: | Combo: | FuncRef: | Index: |\n","|---|:---:|---|---:|\n","**Onset Density** | (only) | [0,0,0] | 0 |\n","&emsp; &emsp; +| Metrical Strength  | [0,1,0] | 1 |\n","&emsp; &emsp; +| Syncopation  | [0,0,1] | 2 |\n","&emsp; &emsp; +| Metrical Strength <br>& Syncopation  | [0,1,1] | 3 |\n","--- | --- | --- | ---\n","**Frequency Weighted <br>Onset Density** | (only) | [1,0,0] | 4 |\n","&emsp; &emsp; +| Metrical Strength  | [1,1,0] | 5 |\n","&emsp; &emsp; +| Syncopation  | [1,0,1] | 6 |\n","&emsp; &emsp; +| Metrical Strength <br>& Syncopation  | [1,1,1] | 7 |\n","--- | --- | --- | ---\n","**Pattern Relative <br>Onset Density** | (only) | [2,0,0] | 8 |\n","&emsp; &emsp; +| Metrical Strength  | [2,1,0] | 9 |\n","&emsp; &emsp; +| Syncopation  | [2,0,1] | 10 |\n","&emsp; &emsp; +| Metrical Strength <br>& Syncopation  | [2,1,1] | 11 |\n","--- | --- | --- | ---\n","**Note Presence** | (only/mono) | [3,0,0] | 12 |\n","&emsp; &emsp; +| Metrical Strength  | [3,1,0] | 13 |\n","&emsp; &emsp; +| Syncopation  | [3,0,1] | 14 |\n","&emsp; &emsp; +| Metrical Strength <br>& Syncopation  | [3,1,1] | 15 |\n","&emsp; &emsp; +| Polyphonic Syncopation  | [3,0,2] | 16 |\n","\n"]},{"cell_type":"code","execution_count":100,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[[1.         0.         0.         0.         0.3351724  0.\n","  0.         0.         0.22951724 0.         0.         0.\n","  0.3351724  0.         0.         0.        ]\n"," [1.         0.         0.         0.         0.3469091  0.\n","  0.         0.         0.512      0.         0.         0.\n","  0.3469091  0.         0.         0.        ]\n"," [1.         0.         0.         0.         0.3351724  0.\n","  0.         0.         0.22951724 0.         0.         0.\n","  0.3351724  0.         0.         0.        ]\n"," [1.         0.         0.         0.         0.3469091  0.\n","  0.         0.         0.512      0.         0.         0.\n","  0.3469091  0.         0.         0.        ]\n"," [1.         0.         0.         0.         0.3351724  0.\n","  0.         0.         0.22951724 0.         0.         0.\n","  0.3351724  0.         0.         0.        ]\n"," [1.         0.         0.         0.         0.3469091  0.\n","  0.         0.         0.512      0.         0.         0.\n","  0.3469091  0.         0.         0.        ]\n"," [1.         0.         0.         0.         0.3351724  0.\n","  0.         0.         0.22951724 0.         0.         0.\n","  0.3351724  0.         0.         0.        ]\n"," [1.         0.         0.         0.         0.3469091  0.\n","  0.         0.         0.512      0.         0.         0.\n","  0.3469091  0.         0.         0.        ]\n"," [1.         0.         0.         0.         0.3351724  0.\n","  0.         0.         0.22951724 0.         0.         0.\n","  0.3351724  0.         0.         0.        ]\n"," [1.         0.         0.         0.         0.3469091  0.\n","  0.         0.         0.512      0.         0.         0.\n","  0.3469091  0.         0.         0.        ]\n"," [1.         0.         0.         0.         0.3351724  0.\n","  0.         0.         0.22951724 0.         0.         0.\n","  0.3351724  0.         0.         0.        ]\n"," [1.         0.         0.         0.         0.3469091  0.\n","  0.         0.         0.512      0.         0.         0.\n","  0.3469091  0.         0.         0.        ]\n"," [1.         0.         0.         0.         0.3351724  0.\n","  0.         0.         0.22951724 0.         0.         0.\n","  0.3351724  0.         0.         0.        ]\n"," [1.         0.         0.         0.         0.288      0.\n","  0.         0.         0.512      0.         0.         0.\n","  0.288      0.         0.         0.        ]\n"," [1.         0.         0.         0.         0.312      0.\n","  0.         0.         0.512      0.         0.         0.\n","  0.312      0.         0.         0.        ]\n"," [1.         0.         0.         0.         0.288      0.\n","  0.         0.         0.512      0.         0.         0.\n","  0.288      0.         0.         0.        ]\n"," [1.         0.         0.         0.         0.312      0.\n","  0.         0.         0.512      0.         0.         0.\n","  0.312      0.         0.         0.        ]\n"," [1.         0.         0.         0.         0.288      0.\n","  0.         0.         0.512      0.         0.         0.\n","  0.288      0.         0.         0.        ]\n"," [1.         0.         0.         0.         0.312      0.\n","  0.         0.         0.512      0.         0.         0.\n","  0.312      0.         0.         0.        ]\n"," [1.         0.         0.         0.         0.288      0.\n","  0.         0.         0.512      0.         0.         0.\n","  0.288      0.         0.         0.        ]]\n"]}],"source":["### SELECT WHICH ALG ###\n","flattern_num = 5\n","\n","input_data = np.asarray(flatterns[:,flattern_num,:],dtype=np.float32)\n","target_data = np.asarray(mds_pos,dtype=np.float32)\n","\n","\n","# make semi continuous\n","_semicontinuous = True\n","if _semicontinuous:\n","    p_means = np.mean(input_data, axis=1, keepdims=True)\n","    input_data[input_data<p_means] = 0.0\n","\n","# make discrete\n","_discrete = False\n","if _discrete:\n","    p_means = np.mean(input_data, axis=1, keepdims=True)\n","    input_data[input_data>=p_means] = 1\n","    input_data[input_data<p_means] = 0\n","\n","print(input_data[220:240])"]},{"cell_type":"code","execution_count":91,"metadata":{},"outputs":[],"source":["## 10 fold cross evaluation splits ##\n","training_DL_list = []\n","validation_DL_list = []\n","\n","seed=np.random.randint(0,1000) ## for traintest splits. \n","train, test, train_coords, test_coords = train_test_split(input_data, target_data, test_size=0.2, random_state=seed)\n","test_dataset = neuralnetwork.CustomDataset(test, test_coords)\n","test_dataloader = tdata.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n","\n","# make train and validation splits on training data\n","folds=10\n","kfold = KFold(n_splits=folds, shuffle=True) \n","\n","for train_idx, validation_idx in kfold.split(train):\n","    \n","    train_fold, validation_fold = input_data[train_idx], input_data[validation_idx]\n","    target_fold, target_val_fold = target_data[train_idx], target_data[validation_idx]\n","\n","    train_dataset = neuralnetwork.CustomDataset(train_fold, target_fold)\n","    validation_dataset = neuralnetwork.CustomDataset(validation_fold, target_val_fold)\n","\n","    train_dataloader = tdata.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","    validation_dataloader = tdata.DataLoader(validation_dataset, batch_size=batch_size, shuffle=False)\n","\n","    training_DL_list.append(train_dataloader)\n","    validation_DL_list.append(validation_dataloader)"]},{"cell_type":"code","execution_count":101,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Using cpu device\n","Fold:0 Avg:0.1377(0.1372) FinalTestLoss:0.017\n","Fold:1 Avg:0.1365(0.1122) FinalTestLoss:0.016\n","Fold:2 Avg:0.1186(0.1206) FinalTestLoss:0.013\n","Fold:3 Avg:0.1073(0.1035) FinalTestLoss:0.010\n","Fold:4 Avg:0.1051(0.1051) FinalTestLoss:0.011\n","Fold:5 Avg:0.0978(0.0917) FinalTestLoss:0.009\n","Fold:6 Avg:0.0909(0.1109) FinalTestLoss:0.012\n","Fold:7 Avg:0.1098(0.1320) FinalTestLoss:0.015\n","Fold:8 Avg:0.1097(0.1305) FinalTestLoss:0.014\n","Fold:9 Avg:0.0998(0.1282) FinalTestLoss:0.015\n","0.0998165130623829\n"]}],"source":["## Train a model on 10-folds for one flattening algorithm ##\n","## This process can take around 35-40sec ##\n","importlib.reload(neuralnetwork)\n","accuracy=[[0.0,0.0] for n in range(len(training_DL_list))] # num splits\n","for split in range(len(training_DL_list)):\n","    dataloader = training_DL_list[split]\n","    val_dataloader = validation_DL_list[split]\n","\n","    neuralnetwork.train_loop(model, dataloader, epochs, criterion, optimizer)\n","    all_acc = []\n","    all_acc, accuracy[split][1] = neuralnetwork.test_loop(model, val_dataloader, criterion)\n","    accuracy[split][0] = np.average(all_acc)\n","    print(f\"Fold:{split} Avg:{accuracy[split][0]:.4f}({np.std(all_acc):.4f}) FinalTestLoss:{accuracy[split][1]:.3f}\")\n","print(f\"{np.average(accuracy[split][0])}\")\n"]},{"cell_type":"code","execution_count":98,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Algorithm: FreqWeightOnsetDen_Meter --> MAE: 0.1037 [std:0.16062075116934466] ***** Test Dataset Only *****\n","Algorithm: FreqWeightOnsetDen_Meter --> MAE: 0.1137 [std:0.157566096145877] ***** Whole Dataset *****\n"]}],"source":["## Evaluate Model ##\n","mean_abs_errors, _ = neuralnetwork.test_loop(model, test_dataloader, criterion)\n","mean_abs_error = np.average(mean_abs_errors)\n","mean_CV = round(np.std(mean_abs_errors)/mean_abs_error,4)\n","print(f\"Algorithm: {flat_names[flattern_num]} --> MAE: {mean_abs_error:.4f} [std:{np.std(mean_abs_errors)}] ***** Test Dataset Only *****\")\n","\n","whole_dataset = neuralnetwork.CustomDataset(input_data, target_data)\n","whole_dataloader = tdata.DataLoader(whole_dataset, batch_size=batch_size, shuffle=False)\n","mean_abs_errors, _ = neuralnetwork.test_loop(model, whole_dataloader, criterion)\n","mean_abs_error = np.average(mean_abs_errors)\n","mean_CV = round(np.std(mean_abs_errors)/mean_abs_error,4)\n","print(f\"Algorithm: {flat_names[flattern_num]} --> MAE: {mean_abs_error:.4f} [std:{np.std(mean_abs_errors)}] ***** Whole Dataset *****\")"]},{"cell_type":"markdown","metadata":{},"source":["The code below will build models for all flattening algorithms, but it will take time to complete. "]},{"cell_type":"code","execution_count":150,"metadata":{"id":"c4901CAyhdnL"},"outputs":[{"name":"stdout","output_type":"stream","text":["Using cpu device\n","Moving on to flat-type: OnsetDen\n","Fold:0 Avg:0.1628 FinalTestLoss:0.023\n","Fold:1 Avg:0.1425 FinalTestLoss:0.017\n","Fold:2 Avg:0.1496 FinalTestLoss:0.021\n","Fold:3 Avg:0.1552 FinalTestLoss:0.022\n","Fold:4 Avg:0.1324 FinalTestLoss:0.017\n","Algorithm: OnsetDen --> MAE: 0.1579 [std:0.13659816229333624] ***** Test Dataset Only *****\n","Algorithm: OnsetDen --> MAE: 0.1535 [std:0.13003744486214106] ***** Whole Dataset *****\n","Moving on to flat-type: OnsetDen_Meter\n","Fold:0 Avg:0.2162 FinalTestLoss:0.033\n","Fold:1 Avg:0.2169 FinalTestLoss:0.031\n","Fold:2 Avg:0.1996 FinalTestLoss:0.025\n","Fold:3 Avg:0.1987 FinalTestLoss:0.027\n","Fold:4 Avg:0.2128 FinalTestLoss:0.030\n","Algorithm: OnsetDen_Meter --> MAE: 0.2265 [std:0.1197344374683496] ***** Test Dataset Only *****\n","Algorithm: OnsetDen_Meter --> MAE: 0.2128 [std:0.1155751288122925] ***** Whole Dataset *****\n","Moving on to flat-type: OnsetDen_Sync\n","Fold:0 Avg:0.0636 FinalTestLoss:0.006\n","Fold:1 Avg:0.0580 FinalTestLoss:0.004\n","Fold:2 Avg:0.0538 FinalTestLoss:0.004\n","Fold:3 Avg:0.0520 FinalTestLoss:0.005\n","Fold:4 Avg:0.0668 FinalTestLoss:0.007\n","Algorithm: OnsetDen_Sync --> MAE: 0.0668 [std:0.0872632028506128] ***** Test Dataset Only *****\n","Algorithm: OnsetDen_Sync --> MAE: 0.0728 [std:0.10241471238982028] ***** Whole Dataset *****\n","Moving on to flat-type: OnsetDen_Sync_Meter\n","Fold:0 Avg:0.1544 FinalTestLoss:0.020\n","Fold:1 Avg:0.1426 FinalTestLoss:0.018\n","Fold:2 Avg:0.1382 FinalTestLoss:0.017\n","Fold:3 Avg:0.1531 FinalTestLoss:0.018\n","Fold:4 Avg:0.1464 FinalTestLoss:0.017\n","Algorithm: OnsetDen_Sync_Meter --> MAE: 0.1527 [std:0.1258380358049451] ***** Test Dataset Only *****\n","Algorithm: OnsetDen_Sync_Meter --> MAE: 0.1658 [std:0.12778412691714106] ***** Whole Dataset *****\n","Moving on to flat-type: FreqWeightOnsetDen\n","Fold:0 Avg:0.1549 FinalTestLoss:0.021\n","Fold:1 Avg:0.1435 FinalTestLoss:0.018\n","Fold:2 Avg:0.1484 FinalTestLoss:0.020\n","Fold:3 Avg:0.1453 FinalTestLoss:0.019\n","Fold:4 Avg:0.1603 FinalTestLoss:0.022\n","Algorithm: FreqWeightOnsetDen --> MAE: 0.1526 [std:0.1350017640277968] ***** Test Dataset Only *****\n","Algorithm: FreqWeightOnsetDen --> MAE: 0.1562 [std:0.14220147570002545] ***** Whole Dataset *****\n","Moving on to flat-type: FreqWeightOnsetDen_Meter\n","Fold:0 Avg:0.2242 FinalTestLoss:0.032\n","Fold:1 Avg:0.2215 FinalTestLoss:0.031\n","Fold:2 Avg:0.2212 FinalTestLoss:0.031\n","Fold:3 Avg:0.2250 FinalTestLoss:0.031\n","Fold:4 Avg:0.2283 FinalTestLoss:0.033\n","Algorithm: FreqWeightOnsetDen_Meter --> MAE: 0.2224 [std:0.10460252757148625] ***** Test Dataset Only *****\n","Algorithm: FreqWeightOnsetDen_Meter --> MAE: 0.2239 [std:0.10706382316418908] ***** Whole Dataset *****\n","Moving on to flat-type: FreqWeightOnsetDen_Sync\n","Fold:0 Avg:0.1507 FinalTestLoss:0.021\n","Fold:1 Avg:0.1461 FinalTestLoss:0.019\n","Fold:2 Avg:0.1402 FinalTestLoss:0.017\n","Fold:3 Avg:0.1464 FinalTestLoss:0.020\n","Fold:4 Avg:0.1443 FinalTestLoss:0.020\n","Algorithm: FreqWeightOnsetDen_Sync --> MAE: 0.1552 [std:0.1393493494136723] ***** Test Dataset Only *****\n","Algorithm: FreqWeightOnsetDen_Sync --> MAE: 0.1562 [std:0.14064225369578376] ***** Whole Dataset *****\n","Moving on to flat-type: FreqWeightOnsetDen_Sync_Meter\n","Fold:0 Avg:0.2261 FinalTestLoss:0.031\n","Fold:1 Avg:0.2188 FinalTestLoss:0.030\n","Fold:2 Avg:0.2243 FinalTestLoss:0.033\n","Fold:3 Avg:0.2258 FinalTestLoss:0.032\n","Fold:4 Avg:0.2235 FinalTestLoss:0.032\n","Algorithm: FreqWeightOnsetDen_Sync_Meter --> MAE: 0.2334 [std:0.11760075558265343] ***** Test Dataset Only *****\n","Algorithm: FreqWeightOnsetDen_Sync_Meter --> MAE: 0.2253 [std:0.10939030329891708] ***** Whole Dataset *****\n","Moving on to flat-type: RelOnsetDen\n","Fold:0 Avg:0.1552 FinalTestLoss:0.023\n","Fold:1 Avg:0.1471 FinalTestLoss:0.021\n","Fold:2 Avg:0.1285 FinalTestLoss:0.016\n","Fold:3 Avg:0.1417 FinalTestLoss:0.018\n","Fold:4 Avg:0.1313 FinalTestLoss:0.016\n","Algorithm: RelOnsetDen --> MAE: 0.1426 [std:0.12376510204374899] ***** Test Dataset Only *****\n","Algorithm: RelOnsetDen --> MAE: 0.1452 [std:0.13187261143552842] ***** Whole Dataset *****\n","Moving on to flat-type: RelOnsetDen_Meter\n","Fold:0 Avg:0.2055 FinalTestLoss:0.032\n","Fold:1 Avg:0.1882 FinalTestLoss:0.024\n","Fold:2 Avg:0.1971 FinalTestLoss:0.026\n","Fold:3 Avg:0.1927 FinalTestLoss:0.024\n","Fold:4 Avg:0.1897 FinalTestLoss:0.025\n","Algorithm: RelOnsetDen_Meter --> MAE: 0.1843 [std:0.1134485742268885] ***** Test Dataset Only *****\n","Algorithm: RelOnsetDen_Meter --> MAE: 0.1873 [std:0.12080273465860811] ***** Whole Dataset *****\n","Moving on to flat-type: RelOnsetDen_Sync\n","Fold:0 Avg:0.1377 FinalTestLoss:0.019\n","Fold:1 Avg:0.1180 FinalTestLoss:0.015\n","Fold:2 Avg:0.1273 FinalTestLoss:0.016\n","Fold:3 Avg:0.1299 FinalTestLoss:0.018\n","Fold:4 Avg:0.1154 FinalTestLoss:0.013\n","Algorithm: RelOnsetDen_Sync --> MAE: 0.1451 [std:0.13560927638094783] ***** Test Dataset Only *****\n","Algorithm: RelOnsetDen_Sync --> MAE: 0.1387 [std:0.13776387131243814] ***** Whole Dataset *****\n","Moving on to flat-type: RelOnsetDen_Sync_Meter\n","Fold:0 Avg:0.1809 FinalTestLoss:0.026\n","Fold:1 Avg:0.1886 FinalTestLoss:0.027\n","Fold:2 Avg:0.1612 FinalTestLoss:0.021\n","Fold:3 Avg:0.1670 FinalTestLoss:0.022\n","Fold:4 Avg:0.1611 FinalTestLoss:0.021\n","Algorithm: RelOnsetDen_Sync_Meter --> MAE: 0.1582 [std:0.12546419843078815] ***** Test Dataset Only *****\n","Algorithm: RelOnsetDen_Sync_Meter --> MAE: 0.1587 [std:0.13125634755389376] ***** Whole Dataset *****\n","Moving on to flat-type: Mono\n","Fold:0 Avg:0.1053 FinalTestLoss:0.011\n","Fold:1 Avg:0.1036 FinalTestLoss:0.009\n","Fold:2 Avg:0.1045 FinalTestLoss:0.010\n","Fold:3 Avg:0.1085 FinalTestLoss:0.013\n","Fold:4 Avg:0.1023 FinalTestLoss:0.009\n","Algorithm: Mono --> MAE: 0.0976 [std:0.09801694715968783] ***** Test Dataset Only *****\n","Algorithm: Mono --> MAE: 0.1050 [std:0.10022386975491918] ***** Whole Dataset *****\n","Moving on to flat-type: Mono_Meter\n","Fold:0 Avg:0.2188 FinalTestLoss:0.030\n","Fold:1 Avg:0.2197 FinalTestLoss:0.030\n","Fold:2 Avg:0.2146 FinalTestLoss:0.028\n","Fold:3 Avg:0.2061 FinalTestLoss:0.027\n","Fold:4 Avg:0.2248 FinalTestLoss:0.031\n","Algorithm: Mono_Meter --> MAE: 0.2116 [std:0.10603856840089768] ***** Test Dataset Only *****\n","Algorithm: Mono_Meter --> MAE: 0.2165 [std:0.10338837685246931] ***** Whole Dataset *****\n","Moving on to flat-type: Mono_Sync\n","Fold:0 Avg:0.1187 FinalTestLoss:0.013\n","Fold:1 Avg:0.1081 FinalTestLoss:0.011\n","Fold:2 Avg:0.1005 FinalTestLoss:0.010\n","Fold:3 Avg:0.0954 FinalTestLoss:0.008\n","Fold:4 Avg:0.1039 FinalTestLoss:0.010\n","Algorithm: Mono_Sync --> MAE: 0.1041 [std:0.09814820168873938] ***** Test Dataset Only *****\n","Algorithm: Mono_Sync --> MAE: 0.1054 [std:0.10302864830569314] ***** Whole Dataset *****\n","Moving on to flat-type: Mono_Sync_Meter\n","Fold:0 Avg:0.2153 FinalTestLoss:0.029\n","Fold:1 Avg:0.2133 FinalTestLoss:0.029\n","Fold:2 Avg:0.2180 FinalTestLoss:0.030\n","Fold:3 Avg:0.2048 FinalTestLoss:0.027\n","Fold:4 Avg:0.2172 FinalTestLoss:0.029\n","Algorithm: Mono_Sync_Meter --> MAE: 0.2145 [std:0.1012783587101413] ***** Test Dataset Only *****\n","Algorithm: Mono_Sync_Meter --> MAE: 0.2148 [std:0.10368398664786628] ***** Whole Dataset *****\n","Moving on to flat-type: Mono_PolySync\n","Fold:0 Avg:0.0881 FinalTestLoss:0.011\n","Fold:1 Avg:0.0830 FinalTestLoss:0.010\n","Fold:2 Avg:0.0706 FinalTestLoss:0.007\n","Fold:3 Avg:0.0781 FinalTestLoss:0.008\n","Fold:4 Avg:0.0782 FinalTestLoss:0.008\n","Algorithm: Mono_PolySync --> MAE: 0.0850 [std:0.11272325114750768] ***** Test Dataset Only *****\n","Algorithm: Mono_PolySync --> MAE: 0.0849 [std:0.11265960931790431] ***** Whole Dataset *****\n"]}],"source":["## For all Flat: NN build + (train + test)-10fold ## WARNING: LONGTIME!\n","# TODO\n","importlib.reload(neuralnetwork)\n","\n","train_scores = []\n","test_scores = []\n","\n","for flat in range(len(flat_names)):\n","    print(f\"Moving on to flat-type: {flat_names[flat]}\" )\n","    ## PREPARE ----------------------------------------\n","    model = neuralnetwork.build_model(len_pattern=16,firstlayer=32,secondlayer=32,thirdlayer=16,fourthlayer=16,coords=2)\n","    ## Select model hyperparameters ##\n","    learning_rate = 0.0015\n","    batch_size = 32\n","    criterion = nn.MSELoss()\n","    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n","    epochs = 300\n","\n","    input_data = np.asarray(flatterns[:,flat,:],dtype=np.float32)\n","    target_data = np.asarray(mds_pos,dtype=np.float32)\n","\n","    # make semi continuous\n","    _semicontinuous = False\n","    if _semicontinuous:\n","        p_means = np.mean(input_data, axis=1, keepdims=True)\n","        input_data[input_data<p_means] = 0.0\n","\n","    # make discrete\n","    _discrete = True\n","    if _discrete:\n","        p_means = np.mean(input_data, axis=1, keepdims=True)\n","        input_data[input_data>=p_means] = 1.0\n","        input_data[input_data<p_means] = 0.0\n","\n","    ## 10 fold cross evaluation splits ##\n","    training_DL_list = []\n","    validation_DL_list = []\n","\n","    seed=np.random.randint(0,1000) ## for traintest splits. \n","    train, test, train_coords, test_coords = train_test_split(input_data, target_data, test_size=0.2, random_state=seed)\n","    test_dataset = neuralnetwork.CustomDataset(test, test_coords)\n","    test_dataloader = tdata.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n","\n","    # make train and validation splits on training data\n","    folds=5\n","    kfold = KFold(n_splits=folds, shuffle=True) \n","\n","    for train_idx, validation_idx in kfold.split(train):\n","        \n","        train_fold, validation_fold = input_data[train_idx], input_data[validation_idx]\n","        target_fold, target_val_fold = target_data[train_idx], target_data[validation_idx]\n","\n","        train_dataset = neuralnetwork.CustomDataset(train_fold, target_fold)\n","        validation_dataset = neuralnetwork.CustomDataset(validation_fold, target_val_fold)\n","\n","        train_dataloader = tdata.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","        validation_dataloader = tdata.DataLoader(validation_dataset, batch_size=batch_size, shuffle=False)\n","\n","        training_DL_list.append(train_dataloader)\n","        validation_DL_list.append(validation_dataloader)\n","\n","    accuracy=[[0.0,0.0] for n in range(len(training_DL_list))] # num splits\n","    for split in range(len(training_DL_list)):\n","        dataloader = training_DL_list[split]\n","        val_dataloader = validation_DL_list[split]\n","\n","        neuralnetwork.train_loop(model, dataloader, epochs, criterion, optimizer)\n","        all_acc = []\n","        all_acc, accuracy[split][1] = neuralnetwork.test_loop(model, val_dataloader, criterion)\n","        accuracy[split][0] = np.average(all_acc)\n","        print(f\"Fold:{split} Avg:{accuracy[split][0]:.4f} FinalTestLoss:{accuracy[split][1]:.3f}\")\n","        train_scores.append(all_acc)\n","\n","    ## Evaluate Model ##\n","    mean_abs_errors, _ = neuralnetwork.test_loop(model, test_dataloader, criterion)\n","    mean_abs_error = np.average(mean_abs_errors)\n","    mean_CV = round(np.std(mean_abs_errors)/mean_abs_error,4)\n","    print(f\"Algorithm: {flat_names[flat]} --> MAE: {mean_abs_error:.4f} [std:{np.std(mean_abs_errors)}] ***** Test Dataset Only *****\")\n","\n","    test_scores.append(mean_abs_errors)\n","    \n","    whole_dataset = neuralnetwork.CustomDataset(input_data, target_data)\n","    whole_dataloader = tdata.DataLoader(whole_dataset, batch_size=batch_size, shuffle=False)\n","    mean_abs_errors, _ = neuralnetwork.test_loop(model, whole_dataloader, criterion)\n","    mean_abs_error = np.average(mean_abs_errors)\n","    mean_CV = round(np.std(mean_abs_errors)/mean_abs_error,4)\n","    print(f\"Algorithm: {flat_names[flat]} --> MAE: {mean_abs_error:.4f} [std:{np.std(mean_abs_errors)}] ***** Whole Dataset *****\")\n","    \n"]},{"cell_type":"code","execution_count":151,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["OnsetDen\n","TEST: 0.1579, (0.1366)\n","OnsetDen_Meter\n","TEST: 0.2265, (0.1197)\n","OnsetDen_Sync\n","TEST: 0.0668, (0.0873)\n","OnsetDen_Sync_Meter\n","TEST: 0.1527, (0.1258)\n","FreqWeightOnsetDen\n","TEST: 0.1526, (0.1350)\n","FreqWeightOnsetDen_Meter\n","TEST: 0.2224, (0.1046)\n","FreqWeightOnsetDen_Sync\n","TEST: 0.1552, (0.1393)\n","FreqWeightOnsetDen_Sync_Meter\n","TEST: 0.2334, (0.1176)\n","RelOnsetDen\n","TEST: 0.1426, (0.1238)\n","RelOnsetDen_Meter\n","TEST: 0.1843, (0.1134)\n","RelOnsetDen_Sync\n","TEST: 0.1451, (0.1356)\n","RelOnsetDen_Sync_Meter\n","TEST: 0.1582, (0.1255)\n","Mono\n","TEST: 0.0976, (0.0980)\n","Mono_Meter\n","TEST: 0.2116, (0.1060)\n","Mono_Sync\n","TEST: 0.1041, (0.0981)\n","Mono_Sync_Meter\n","TEST: 0.2145, (0.1013)\n","Mono_PolySync\n","TEST: 0.0850, (0.1127)\n"]}],"source":["# some data rearranging\n","trn_scores = []\n","train_means = np.array([[0.0 for x in range(folds)] for y in range(len(flat_names))], dtype=float)\n","train_stds = np.array([[0.0 for x in range(folds)] for y in range(len(flat_names))], dtype=float)\n","for f in range(len(flat_names)):\n","    t_s = []\n","    for g in range(folds):\n","        t_s.append(train_scores[f*g+g])\n","        train_means[f][g]=np.mean(np.asarray(train_scores[f*g+g],dtype=float))\n","        train_stds[f][g]=np.std(np.asarray(train_scores[f*g+g],dtype=float))\n","    trn_scores.append(t_s)\n","\n","for a in range(len(flat_names)):\n","    print(f\"{flat_names[a]}\")\n","    train_mean = np.mean(train_means[a])\n","    train_std = np.mean(train_stds[a])\n","    #print(f\"TRAIN: {train_mean:.4f}, {train_std:.4f}\")\n","    print(f\"TEST: {np.mean(test_scores[a]):.4f}, ({np.std(test_scores[a]):.4f})\")"]},{"cell_type":"markdown","metadata":{},"source":["### 300 0.0015 cont ###\n","\n","OnsetDen\n","TEST: 0.0436, (0.0778)\n","\n","OnsetDen_Meter\n","TEST: 0.0438, (0.0934)\n","\n","OnsetDen_Sync\n","TEST: 0.0444, (0.0878)\n","\n","OnsetDen_Sync_Meter\n","TEST: 0.0366, (0.0802)\n","\n","FreqWeightOnsetDen\n","TEST: 0.0391, (0.0906)\n","\n","FreqWeightOnsetDen_Meter\n","TEST: 0.0517, (0.0838)\n","\n","FreqWeightOnsetDen_Sync\n","TEST: 0.0480, (0.0954)\n","\n","FreqWeightOnsetDen_Sync_Meter\n","TEST: 0.0539, (0.0923)\n","\n","RelOnsetDen\n","TEST: 0.0448, (0.0752)\n","\n","RelOnsetDen_Meter\n","TEST: 0.0406, (0.0784)\n","\n","RelOnsetDen_Sync\n","TEST: 0.0560, (0.1062)\n","\n","RelOnsetDen_Sync_Meter\n","TEST: 0.0543, (0.1111)\n","\n","Mono\n","TEST: 0.1042, (0.1010)\n","\n","Mono_Meter\n","TEST: 0.1125, (0.1045)\n","\n","Mono_Sync\n","TEST: 0.0971, (0.0937)\n","\n","Mono_Sync_Meter\n","TEST: 0.0970, (0.0966)\n","\n","Mono_PolySync\n","TEST: 0.0672, (0.1084)\n","\n","\n","### 300 0.0015 semi-cont ###\n","\n","OnsetDen\n","TEST: 0.0832, (0.1357)\n","\n","OnsetDen_Meter\n","TEST: 0.1085, (0.1317)\n","\n","OnsetDen_Sync\n","TEST: 0.0526, (0.1068)\n","\n","OnsetDen_Sync_Meter\n","TEST: 0.0777, (0.1180)\n","\n","FreqWeightOnsetDen\n","TEST: 0.0585, (0.1041)\n","\n","FreqWeightOnsetDen_Meter\n","TEST: 0.1656, (0.1219)\n","\n","FreqWeightOnsetDen_Sync\n","TEST: 0.0639, (0.1156)\n","\n","FreqWeightOnsetDen_Sync_Meter\n","TEST: 0.1360, (0.1623)\n","\n","RelOnsetDen\n","TEST: 0.0808, (0.1217)\n","\n","RelOnsetDen_Meter\n","TEST: 0.1211, (0.1304)\n","\n","RelOnsetDen_Sync\n","TEST: 0.0739, (0.1390)\n","\n","RelOnsetDen_Sync_Meter\n","TEST: 0.0714, (0.1393)\n","\n","Mono\n","TEST: 0.1115, (0.1048)\n","\n","Mono_Meter\n","TEST: 0.2247, (0.1064)\n","\n","Mono_Sync\n","TEST: 0.1081, (0.1081)\n","\n","Mono_Sync_Meter\n","TEST: 0.2142, (0.1151)\n","\n","Mono_PolySync\n","TEST: 0.0749, (0.1132)\n","\n","### 300 0.0015 discrete ###\n","\n","OnsetDen\n","TEST: 0.1579, (0.1366)\n","\n","OnsetDen_Meter\n","TEST: 0.2265, (0.1197)\n","\n","OnsetDen_Sync\n","TEST: 0.0668, (0.0873)\n","\n","OnsetDen_Sync_Meter\n","TEST: 0.1527, (0.1258)\n","\n","FreqWeightOnsetDen\n","TEST: 0.1526, (0.1350)\n","\n","FreqWeightOnsetDen_Meter\n","TEST: 0.2224, (0.1046)\n","\n","FreqWeightOnsetDen_Sync\n","TEST: 0.1552, (0.1393)\n","\n","FreqWeightOnsetDen_Sync_Meter\n","TEST: 0.2334, (0.1176)\n","\n","RelOnsetDen\n","TEST: 0.1426, (0.1238)\n","\n","RelOnsetDen_Meter\n","TEST: 0.1843, (0.1134)\n","\n","RelOnsetDen_Sync\n","TEST: 0.1451, (0.1356)\n","\n","RelOnsetDen_Sync_Meter\n","TEST: 0.1582, (0.1255)\n","\n","Mono\n","TEST: 0.0976, (0.0980)\n","\n","Mono_Meter\n","TEST: 0.2116, (0.1060)\n","\n","Mono_Sync\n","TEST: 0.1041, (0.0981)\n","\n","Mono_Sync_Meter\n","TEST: 0.2145, (0.1013)\n","\n","Mono_PolySync\n","TEST: 0.0850, (0.1127)\n"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyPe6IdBNzwQcOsCeGsJDX66","mount_file_id":"1fp35ZDb8z0dUm6llW0_UE3L5HPNugD6E","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.6"}},"nbformat":4,"nbformat_minor":0}
